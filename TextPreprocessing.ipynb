{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Process of cleaning the unstructured text data in order to make it noise-free and ready for further analysis.\n",
    " - Load raw data\n",
    " - Tokenize text\n",
    " - Filter out tokens that are stop words.\n",
    " - Convert to lowercase.\n",
    " - Remove punctuation marks, URLs and special characters from each token.\n",
    " - Stemming\n",
    " - Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages used for this session: \n",
    "- NLTK (Natural Language Toolkit)\n",
    "- spaCy (library for advanced NLP)\n",
    "\n",
    "## Install NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk #install NLTK and run in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install nltk #if working with jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download NLTK data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Word Removal \n",
    "\n",
    "NLTK has a list of stopwords in 16 different languages. Stopwords are usually referred to as noisy words in a language, i.e., words with higher frequency, that a search engine has been programmed to ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#stopword removal\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#specifying the stopword language dataset\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words) #printing all stopwords in the English stopwords dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopword removal from a string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize #word_tokenize accepts a string as an input, not a file\n",
    "example_line='something...is! wrong() with.,; this :: sentence.'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(example_line)\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "filtered_sentence = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopword removal from a text file\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#read from a file as a stream\n",
    "filename = open('preprocessing_data.txt')\n",
    "line = filename.read()\n",
    "words = line.split()\n",
    "for r in words:\n",
    "    if r not in stop_words:\n",
    "        outputFile = open('nostopwords_data.txt','a') #writing processed output to a new file\n",
    "        outputFile.write(\" \"+r)\n",
    "        outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to lowercase\n",
    "with open('preprocessing_data.txt','r') as fileinput:\n",
    "    for line in fileinput:\n",
    "        line = line.lower()\n",
    "        with open('lowercase_data.txt','a') as outputFile: #writing processed output to a new file\n",
    "            outputFile.writelines(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing punctuations, URL and special characters\n",
    "import string\n",
    "tr = str.maketrans(\"\",\"\",string.punctuation)\n",
    "s = \"{{Here is some stuff in curly braces..!!}}\"\n",
    "s.translate(tr)\n",
    "\n",
    "with open('preprocessing_data.txt','r') as fileinput:\n",
    "    for line in fileinput:\n",
    "        stripped = line.translate(tr)\n",
    "        print(stripped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence segmentation\n",
    "from nltk import tokenize\n",
    "with open('preprocessing_data.txt','r') as fileinput:\n",
    "    for line in fileinput:\n",
    "        tokenized = tokenize.sent_tokenize(line)\n",
    "        print(tokenized)\n",
    "        \n",
    "#text tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "with open('preprocessing_data.txt','r') as fileinput:\n",
    "    for line in fileinput:\n",
    "        tokens = word_tokenize(line)\n",
    "        print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    " -  Reducing each word to its root or base.\n",
    " -  A rudimentary rule-based process of stripping the suffixes (\"ing\", \"ly\", \"es\", \"s\" etc) from a word.\n",
    " -  For example \"fishing\", \"fished\", \"fisher\" all reduce to the stem \"fish\". \n",
    " -  \"studies\" --> \"studi\", \"studying\" --> \"study\"\n",
    " -  Most common algorithm for stemming English is Porter's algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "filename = open('preprocessing_data.txt')\n",
    "line = filename.read()\n",
    "tokens = word_tokenize(line)\n",
    "#import the stemming algorithm: Porter's Stemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in tokens]\n",
    "print(stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    " - It usually refers to the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word known as the lemma.\n",
    " - \"studies\", \"studying\" --> \"study\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "filename = open('preprocessing_data.txt')\n",
    "line = filename.read()\n",
    "tokens = word_tokenize(line)\n",
    "#import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "lemmatized = [lem.lemmatize(word,'v') for word in tokens]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tagging\n",
    "\n",
    "Identifying lexical units within text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "with open('preprocessing_data.txt','r') as fileinput:\n",
    "    for line in fileinput:\n",
    "        tokens = word_tokenize(line)\n",
    "        tags = nltk.pos_tag(tokens)\n",
    "        print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "Information Extraction task to identify and classify named entities found in text into pre-defined entity types such as PERSON, LOCATION, ORG, etc.\n",
    "\n",
    "<b>Entity Identification</b> is performed using dependency parsing and part-of-speech tagging (noun phrases).<br>\n",
    "<b>Entity Classification</b> deals with categorizing the identified noun phrases into various types which can be performed using lookup of type dictionaries and other sources (Wikipedia, DBpedia, Google Maps,..) available on the Web.\n",
    "\n",
    "\n",
    "Entity types used by spaCy are listed <a href=\"https://spacy.io/api/annotation#named-entities\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm #The installation of spaCy doesn’t automatically download the English model.\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "doc = nlp(\"Next week I'll be in Madrid.\") #to obtain IOB style tagging of sentences with the entity types\n",
    "iob_tagged = [\n",
    "    (\n",
    "        token.text, \n",
    "        token.tag_, \n",
    "        \"{0}-{1}\".format(token.ent_iob_, token.ent_type_) if token.ent_iob_ != 'O' else token.ent_iob_\n",
    "    ) for token in doc\n",
    "]\n",
    " \n",
    "print(iob_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining frequencies of entity types\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Monica, Mary and Oliver had lunch together and bought some Apple products.\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    \n",
    "lables = [x.label_ for x in doc.ents]\n",
    "Counter(lables) #count entity type occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualising the document with identified named entities and entity types.\n",
    "displacy.render(doc, jupyter=True, style='ent')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
